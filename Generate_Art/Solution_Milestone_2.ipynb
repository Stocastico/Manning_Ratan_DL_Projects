{"cells":[{"cell_type":"markdown","metadata":{"id":"uLzHIySHa1MN"},"source":["# Milestone 2 - Visualising Filter Maximizations, Grad-CAM and Class Maximization\n","\n","__Objective__: \n","- Visualize filter maximizations of a pretrained CNN. The purpose is to get experience in performing gradient ascent and utilizing Keras’s backend functions.\n","- Visualize what areas in an image activate a CNN’s decision-making process using Gradient-weighted Class Activation Mapping (Grad-CAM).\n","- Visualize the input that maximizes specific classes to gain insight into what a CNN thinks each class looks like. \n","\n","\n","__Workflow__:\n","1.  We are going to learn how to maximize convolution filters.\n","    - Load a pretrained (on imagenet) VGG16 model without the top layer\n","    - Extract the layer output for the `block3_conv1` convolution block using, `model.get_layer(layer_name).output`.\n","    - Create a variable called `filter_index`, this will be the filter you wish to visualize (0 to 512) in that layer.\n","2. We are going to start building a loss function that seeks to maximize the activation of an individual filter of your choice. This allows us to visualize what CNN’s filters are detecting or looking for in order to be activated. We start to do this by using the Keras backend utility to calculate the loss. The loss is given by `K.mean(layer_output[:, :, :, filter_index])` which gives us the mean of the tensor that is indexed by the filter you wish to maximize, `filter index`.\n","3.  To keep track of our losses, we use the Keras gradients function to compute the gradient of the input picture with respect to this loss by using `grads = K.gradients(loss, model.input)[0]`\n","    - Normalize those gradients by dividing by its L2 Norm (the square root of the average of the square of the values in the tensor)\n","4.  Now compute the value of the loss tensor and the gradient tensor for a given input image.\n","    - Use the Keras backend function to do this `iterate = K.function([model.input], [loss, grads])`\n","5. Use a loop to perform stochastic gradient descent for 50 steps\n","    - Firstly, create a blank random color image of dimensions 150 x 150 with some random noise injected\n","    - Use your `iterate` function to obtain the loss value and gradient value \n","    - Update the input image using the gradient values for each step\n","6. The resulting image tensor is a floating-point tensor of shape `(1, 150, 150, 3)`, with values that may not be integers within `[0, 255]`. Hence, you need to post process this tensor to turn it into a displayable image. Create a deprocess function to do so.\n","    - Plot the image pattern that maximizes this filter \n","    - Build this entire process into a simple function and plot a few different filters and experiment using different convolution layers\n","\n","**Note** the interesting patterns filters learn. It can be seen that filters get increasingly complex and refined as you go higher in the model. The filters in the lower layers in the model (block1_conv1) encode simple directional edges and colors (or colored edges, in some cases). The filters from block2_conv1 encode simple textures made from combinations of edges and colors. Finally, the filters in the upper layers show more complex patterns such as feathers, leaves, eyes,  whiskers, and other more structured outlines. This information stored in filters is known as the **Latent Space**. It is the information that is stored internally in a CNN. Remember CNN's work by learning the features of data and simplifying data representations for the purpose of finding patterns.\n","\n","7. Now let's implement the **Gradient-weighted Class Activation Mapping (Grad-CAM)** algorithm. This allows us to visualize what areas in an image activate a CNN’s decision-making process.\n","    - Load the Xception model pretrained on imagenet \n","    - Store the names of the last convolution layer and the classifier layer names.\n","8. Build a function that generates the Gradcam heatmap using the following procedure:\n","   - Create a model that maps the input image to the activations of the last conv layer\n","   - Create a model that maps the activations of the last conv layer to the final class predictions\n","   - Compute the gradient (using TF2.0's gradient tape or otherwise) of the top predicted class for our input image with respect to the activations of the last conv layer\n","   - Using the gradient obtained, create a vector where each entry is the mean intensity of the gradient over a specific feature map (hint using `tf.reduce_mean(grads, axis=(0, 1, 2))` where `grads` is the gradient computed in the previous step.\n","   -  Multiply each channel in the feature map array by \"how important this channel is\" with regard to the top predicted class\n","   - Use the channel-wise mean of the resulting feature map as the heatmap of the class activation (you may need to normalize the results between 0 and 1 for visualizing  alizng this heatmap).\n","9. Use your Xception model to predict the class of your test image and then use the function you created in Step 8 to display the heat map.\n","10. Overlay this heatmap onto the test image to see which areas in the image were most important in activating this class prediction. \n","11. Now we move onto the final piece of our Convolutional Neural Network lesson where we go about finding an input that maximizes a specific class. This allows us to see what our CNN thinks a class looks like.\n","12. Firstly, to visualize activation over final dense layer outputs, we need to switch the softmax activation out for linear since the gradient of the output node will depend on all the other node activations. Doing this in Keras is tricky, so using the `keras-vis` module, use `utils.apply_modifications` to modify network parameters and rebuild the graph.\n","    - Load a pretrained (imagent) VGG16 model without the top\n","    - Extract the last layer of the CNN using the index `[-1]` or using `utils.find_layer_idx(model, 'predictions')` to find the index\n","    - Swap the softmax with linear activations.\n","13. Use the `visualize_activation`function from the `keras-vis` module to visualize the input that corresponds to any class of your choice e.g. class 22 corresponds to a Bald Eagle, class 94 is a hummingbird, 235 is a German Shepard dog etc.)\n","    - Increase the number of iterations to produce a cleaner output as it converges (i.e. we lower the ActivationMax Loss which is a loss used to ask the reverse question. For instance, what kind of input image would increase the networks confidence, for the class representing a bird class? This helps determine what the network might be internalizing as being the 'bird' image space. We don't want our CNN interpreting things such as the leaves of trees to be an indicator of birds, though the association makes some sense it is incorrect). \n","    - Use the `Jitter` input modifier to get a crisper  image, this can import likes this: `from vis.input_modifiers import Jitter`\n","\n","\n","\n","__Purpose__:\n","\n","The purpose of this lesson was to understand how we can maximize filter activations and visualize them. You would have learned how to perform gradient ascent to do this. You would have also learnt how to use the Grad-CAM algorithm to see what areas in an image activate a  CNN’s decision making process and lastly, you would have learnt how to visualize the input that maximizes specific classes. This enables you to gain insight into what a CNN thinks each class looks like. \n","\n","__Deliverable__:\n","\n","The deliverable is a Jupyter Notebook documenting your workflow as you take your first create a function to visualize the filter maximizations of a pretrained CNN. Next, you are to display the Grad-CAM heatmap visuals to see what areas influenced a CNN’s decision and lastly you are to display the inputs that maximize certain classes of your choice using a pretrained (imagenet) CNN such as VGG16."]},{"cell_type":"markdown","metadata":{"id":"3y0ZE5wEznRx"},"source":["\n","# Maximizing Filter Activations\n","\n","The process is simple: you’ll build a loss function that maximizes the value of a given filter in a given convolution layer, and then you’ll use stochastic gradient descent to adjust the values of the input image so as to maximize this activation value. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7099,"status":"ok","timestamp":1611532673732,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"sbonkmjaqO-V","outputId":"da747a54-c57e-480a-d4c6-7b7e7cf82dd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"]}],"source":["%tensorflow_version 1.x\n","from tensorflow.keras.applications import VGG16\n","from keras import backend as K\n","\n","# build the VGG16 network\n","model = VGG16(include_top = False,\n","              weights = 'imagenet')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7049,"status":"ok","timestamp":1611532673733,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"4V43ZzhM9x5T","outputId":"2a38f3d7-6bea-4d5c-d0c7-45a55ce14461"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, None, None, 3)]   0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7047,"status":"ok","timestamp":1611532673733,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"TG3RtlNz9xO0"},"outputs":[],"source":["# We'll explore thet third Convolution block\n","layer_name = 'block3_conv1'\n","\n","# can be any integer from 0 to 511, as there are 512 filters in that layer\n","filter_index = 0\n","\n","# build a loss function that maximizes the activation\n","# of the nth filter of the layer considered\n","# we'll use the get_layer function\n","layer_output = model.get_layer(layer_name).output\n","loss = K.mean(layer_output[:, :, :, filter_index])"]},{"cell_type":"markdown","metadata":{"id":"i-LOKH-g-ZbT"},"source":["To implement gradient descent, you’ll need the gradient of this loss with respect to the model’s input. To do this, you’ll use the gradients function packaged with the backend module of Keras."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"elapsed":7425,"status":"error","timestamp":1611532674118,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"7Nt_GDECu_fC","outputId":"1f301ec0-7f13-4e2b-ce4c-d31f54f258b9"},"outputs":[{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-4-0188195ccde9\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute the gradient of the input picture wrt this loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   4101\u001b[0m   \"\"\"\n\u001b[1;32m   4102\u001b[0m   return gradients_module.gradients(\n\u001b[0;32m-\u003e 4103\u001b[0;31m       loss, variables, colocate_gradients_with_ops=True)\n\u001b[0m\u001b[1;32m   4104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 172\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    173\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 492\u001b[0;31m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[1;32m    493\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[1;32m    494\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."]}],"source":["# compute the gradient of the input picture wrt this loss\n","grads = K.gradients(loss, model.input)[0]"]},{"cell_type":"markdown","metadata":{"id":"zYIz5YGx-fb9"},"source":["A non-obvious trick to use to help the gradient-descent process go smoothly is to normalize the gradient tensor by dividing it by its L2 norm (the square root of the average of the square of the values in the tensor). This ensures that the magnitude of the updates done to the input image is always within the same range."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7411,"status":"aborted","timestamp":1611532674106,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"mdlKrSRcvC4a"},"outputs":[],"source":["# normalization trick: we normalize the gradient\n","grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)"]},{"cell_type":"markdown","metadata":{"id":"NlSQZei1-qNW"},"source":["Now you need a way to compute the value of the loss tensor and the gradient tensor, given an input image. You can define a Keras backend function to do this: iterate is a function that takes a Numpy tensor (as a list of tensors of size 1) and returns a list of two Numpy tensors: the loss value and the gradient value."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7410,"status":"aborted","timestamp":1611532674107,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"PfBuyLFYyDZO"},"outputs":[],"source":["# this function returns the loss and grads given the input picture\n","iterate = K.function([model.input], [loss, grads])"]},{"cell_type":"markdown","metadata":{"id":"W01HvsfJ--G_"},"source":["Now we can define a Python loop to do stochastic gradient ascent."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7408,"status":"aborted","timestamp":1611532674107,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"xLhPEsPVqRcV"},"outputs":[],"source":["import numpy as np\n","\n","# we start from a gray image with some noise\n","img_width, img_height = 150, 150\n","input_img_data = np.random.random((1, img_width, img_height, 3)) * 20 + 128.\n","\n","step = 1.\n","\n","# run gradient ascent for 50 steps\n","for i in range(50):\n","     loss_value, grads_value = iterate([input_img_data])\n","     input_img_data += grads_value * step"]},{"cell_type":"markdown","metadata":{"id":"tYZaxydeAJlq"},"source":["## Deprocess Function\n","\n","The resulting image tensor is a floating-point tensor of shape (1, 150, 150, 3), with values that may not be integers within [0, 255]. Hence, you need to postprocess this tensor to turn it into a displayable image. You do so with the following straightforward utility function."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7408,"status":"aborted","timestamp":1611532674109,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"Tt0NOcicyORb"},"outputs":[],"source":["# util function to convert a tensor into a valid image\n","def deprocess_image(x):\n","    # normalize tensor: center on 0., ensure std is 0.1\n","    x -= x.mean()\n","    x /= (x.std() + 1e-5)\n","    x *= 0.1\n","\n","    # clip to [0, 1]\n","    x += 0.5\n","    x = np.clip(x, 0, 1)\n","\n","    # convert to RGB array\n","    x *= 255\n","    x = np.clip(x, 0, 255).astype('uint8')\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"50ZYLICpAfTr"},"source":["Now you have all the pieces. Let’s plot the image tensor representing the pattern that maximizes the activation of the specified filter."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7406,"status":"aborted","timestamp":1611532674109,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"Fzi-ZlEczI-T"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","img = input_img_data[0]\n","\n","plt.imshow(deprocess_image(img))"]},{"cell_type":"markdown","metadata":{"id":"NLF_Xr1ZiFR5"},"source":["The above filter seems to be looking for polkadot patterns, how interesting!"]},{"cell_type":"markdown","metadata":{"id":"5vbGOwc4FaQ_"},"source":["# We can create a simple function for this entire process"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7405,"status":"aborted","timestamp":1611532674110,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"xXUweCq5ziKt"},"outputs":[],"source":["def generate_pattern(layer_name, filter_index, size=150):\n","    layer_output = model.get_layer(layer_name).output\n","    loss = K.mean(layer_output[:, :, :, filter_index])\n","    grads = K.gradients(loss, model.input)[0]\n","    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n","    iterate = K.function([model.input], [loss, grads])\n","    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n","    step = 1.\n","    for i in range(40):\n","        loss_value, grads_value = iterate([input_img_data])\n","        input_img_data += grads_value * step\n","    img = input_img_data[0]\n","    return deprocess_image(img)\n","\n","# Select any filter you wish to visualize\n","plt.imshow(generate_pattern('block3_conv3', 22))"]},{"cell_type":"markdown","metadata":{"id":"RDhEgBFJiPMM"},"source":["The above pattern is well defined, but a bit hard to interpret what it actually is looking for. Perhaps it's a snake like pattern.\n","\n","It's now important to realize a CNN like VGG16 has almost 1500 individual CONV filters, many different combinations of filter patterns need to be activated in order to indicate a specific class has been identified."]},{"cell_type":"markdown","metadata":{"id":"ys1-6E5tDIvT"},"source":["# Implementing Grad-CAM"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7404,"status":"aborted","timestamp":1611532674110,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"gRlXlIVkDGt5"},"outputs":[],"source":["# import our necessary libaries\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# Display\n","from IPython.display import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm"]},{"cell_type":"markdown","metadata":{"id":"jawWmdy2DWzB"},"source":["### Let's use the Xception model to demonstate Grad-CAM"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7402,"status":"aborted","timestamp":1611532674111,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"28Hb8XnbDVDP"},"outputs":[],"source":["# We load our xception model \n","model_builder = keras.applications.xception.Xception\n","\n","# define the input size \n","img_size = (299, 299)\n","\n","# extract the pre_process input and the decode predict functions\n","preprocess_input = keras.applications.xception.preprocess_input\n","decode_predictions = keras.applications.xception.decode_predictions\n","\n","# Store the names of the last convolution layer and the classifier layer names\n","last_conv_layer_name = \"block14_sepconv2_act\"\n","classifier_layer_names = [\n","    \"avg_pool\",\n","    \"predictions\",\n","]"]},{"cell_type":"markdown","metadata":{"id":"x3pV6L28HgXo"},"source":["### Point to our image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"16yKRBT9P9ZaAbkNRebym3BqS4NWntWu6"},"executionInfo":{"elapsed":7401,"status":"aborted","timestamp":1611532674112,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"-w1DfpXlErh1","outputId":"c1be3cd9-c8e3-41d8-90b6-d0e3a17a04d4"},"outputs":[],"source":["# The local path to our target image\n","img_path = keras.utils.get_file(\n","    \"african_elephant.jpg\", \" https://i.imgur.com/Bvro0YD.png\"\n",")\n","\n","display(Image(img_path))"]},{"cell_type":"markdown","metadata":{"id":"xIYg8KY5HlIW"},"source":["### Helper function that loads our image in the right format"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":7400,"status":"aborted","timestamp":1611532674112,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"kxbV0i9sGbPi"},"outputs":[],"source":["def get_img_array(img_path, size):\n","    # `img` is a PIL image of size 299x299\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` is a float32 Numpy array of shape (299, 299, 3)\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size (1, 299, 299, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array"]},{"cell_type":"markdown","metadata":{"id":"qJgmS4m5HvGU"},"source":["## Our Grad-CAM Heatmap function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":7399,"status":"aborted","timestamp":1611532674113,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"Qpz6ePS9Hubl"},"outputs":[],"source":["def make_gradcam_heatmap(\n","    img_array, model, last_conv_layer_name, classifier_layer_names\n","):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer\n","    last_conv_layer = model.get_layer(last_conv_layer_name)\n","    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n","\n","    # Second, we create a model that maps the activations of the last conv\n","    # layer to the final class predictions\n","    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n","    x = classifier_input\n","    for layer_name in classifier_layer_names:\n","        x = model.get_layer(layer_name)(x)\n","    classifier_model = keras.Model(classifier_input, x)\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        # Compute activations of the last conv layer and make the tape watch it\n","        last_conv_layer_output = last_conv_layer_model(img_array)\n","        tape.watch(last_conv_layer_output)\n","        # Compute class predictions\n","        preds = classifier_model(last_conv_layer_output)\n","        top_pred_index = tf.argmax(preds[0])\n","        top_class_channel = preds[:, top_pred_index]\n","\n","    # This is the gradient of the top predicted class with regard to\n","    # the output feature map of the last conv layer\n","    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n","    pooled_grads = pooled_grads.numpy()\n","    for i in range(pooled_grads.shape[-1]):\n","        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n","\n","    # The channel-wise mean of the resulting feature map\n","    # is our heatmap of class activation\n","    heatmap = np.mean(last_conv_layer_output, axis=-1)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 \u0026 1\n","    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n","    return heatmap"]},{"cell_type":"markdown","metadata":{"id":"crgRX2kbH8UI"},"source":["### Displaying the Heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7397,"status":"aborted","timestamp":1611532674113,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"qfvwZqsfHvpZ"},"outputs":[],"source":["# Prepare image\n","img_array = preprocess_input(get_img_array(img_path, size=img_size))\n","\n","# Make model\n","model = model_builder(weights=\"imagenet\")\n","\n","# Print what the top predicted class is\n","preds = model.predict(img_array)\n","print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n","\n","# Generate class activation heatmap\n","heatmap = make_gradcam_heatmap(\n","    img_array, model, last_conv_layer_name, classifier_layer_names\n",")\n","\n","# Display heatmap\n","plt.matshow(heatmap)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HfcjR859IBEq"},"source":["## Overlaying the heatmap onto the original input image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7396,"status":"aborted","timestamp":1611532674114,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"7fWCQDPaH--_"},"outputs":[],"source":["# We load the original image\n","img = keras.preprocessing.image.load_img(img_path)\n","img = keras.preprocessing.image.img_to_array(img)\n","\n","# We rescale heatmap to a range 0-255\n","heatmap = np.uint8(255 * heatmap)\n","\n","# We use jet colormap to colorize heatmap\n","jet = cm.get_cmap(\"jet\")\n","\n","# We use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap]\n","\n","# We create an image with RGB colorized heatmap\n","jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = jet_heatmap * 0.4 + img\n","superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","\n","# Save the superimposed image\n","save_path = \"elephant_cam.jpg\"\n","superimposed_img.save(save_path)\n","\n","# Display Grad CAM\n","display(Image(save_path))"]},{"cell_type":"markdown","metadata":{"id":"hiaZjq8pIIe-"},"source":["# Finding an input that maximizes a specific class"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7394,"status":"aborted","timestamp":1611532674114,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"M_vmVHktIFNe"},"outputs":[],"source":["# You might need to downgrade scipy to for compatability with keras-vis\n","!pip install scipy==1.1.0"]},{"cell_type":"markdown","metadata":{"id":"p72a4G1kIcOu"},"source":["# Finding an input that maximizes a specific classn VGGNet\n","\n","## Dense Layer Visualizations\n","To visualize activation over final dense layer outputs, we need to switch the softmax activation out for linear since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide utils.apply_modifications to modify network parameters and rebuild the graph.\n","\n","If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear' and compare what happens if we dont do this at the end.# "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7394,"status":"aborted","timestamp":1611532674115,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"JzjAeNdOIPrh"},"outputs":[],"source":["from keras.applications import VGG16\n","from vis.utils import utils\n","from keras import activations\n","\n","# Build the VGG16 network with ImageNet weights\n","model = VGG16(weights='imagenet', include_top=True)\n","\n","# Utility to search for layer index by name. \n","# Alternatively we can specify this as -1 since it corresponds to the last layer.\n","layer_idx = utils.find_layer_idx(model, 'predictions')\n","\n","# Swap softmax with linear\n","model.layers[layer_idx].activation = activations.linear\n","model = utils.apply_modifications(model)"]},{"cell_type":"markdown","metadata":{"id":"8bhSdTIDIeY-"},"source":["## Visualizing a specific Class\n","\n","Let's attempt to visualize a specific class. We'll try to see what our CNN thinks a Bald Eagle (class 22) looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7392,"status":"aborted","timestamp":1611532674115,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"MwAWTe_5Ic9z"},"outputs":[],"source":["from vis.visualization import visualize_activation\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (18, 6)\n","\n","# 22 is the imagenet category for 'bald eagle'\n","img = visualize_activation(model, layer_idx, filter_indices=22)\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"5LxQgljPJBF0"},"source":["#### We can just about see things that resemble a bald eagle in this picture, let us clean it up a bit more by increasing the number of iterations"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7391,"status":"aborted","timestamp":1611532674116,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"VwCjY0PlIwPA"},"outputs":[],"source":["# 22 is the imagenet category for 'bald eagle'\n","img = visualize_activation(model, layer_idx, filter_indices=22, max_iter=500, verbose=True)\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"SwICUOBzJb0L"},"source":["### It's cleaner, now let's use the input modifier jitter to generate a more crisp image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7390,"status":"aborted","timestamp":1611532674116,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"l1cmCE0pJpcU"},"outputs":[],"source":["from vis.input_modifiers import Jitter\n","\n","# 22 is the imagenet category for 'bald eagle'\n","# We set Jitter 16 pixels along all dimensions to during the optimization process.\n","img = visualize_activation(model, layer_idx, filter_indices=22, max_iter=500, input_modifiers=[Jitter(16)])\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"2r3BQCaKJ4Eo"},"source":["### Try this for different classes in the imagenet pretrained models to experiment!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7389,"status":"aborted","timestamp":1611532674117,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"HI-nFj50Zk0E"},"outputs":[],"source":["pip freeze \u003e requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7388,"status":"aborted","timestamp":1611532674117,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":240},"id":"4DBvFSG4ZlYg"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"2_1_Solution_Milestone_1_Visualising_Filter_Maximizations_GradCAM_and Class_Maximization.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}